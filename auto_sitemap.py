#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ™ºèƒ½ç½‘ç«™åœ°å›¾ç”Ÿæˆå™¨
è‡ªåŠ¨è§£æHTMLæ–‡ä»¶å¹¶ç”Ÿæˆå®Œæ•´çš„sitemap.xml
æ”¯æŒè‡ªåŠ¨æ£€æµ‹é¡µé¢å†…å®¹å’Œæ›´æ–°é¢‘ç‡
"""

import xml.etree.ElementTree as ET
from datetime import datetime
import os
import re
from bs4 import BeautifulSoup
import json

class SitemapGenerator:
    def __init__(self, base_url="https://yourdomain.com", html_file="index.html"):
        self.base_url = base_url.rstrip('/')
        self.html_file = html_file
        self.current_time = datetime.now().strftime("%Y-%m-%dT%H:%M:%S+00:00")
        
    def parse_html_content(self):
        """è§£æHTMLæ–‡ä»¶ï¼Œæå–é¡µé¢ä¿¡æ¯"""
        if not os.path.exists(self.html_file):
            print(f"âŒ HTMLæ–‡ä»¶ {self.html_file} ä¸å­˜åœ¨")
            return []
        
        try:
            with open(self.html_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            soup = BeautifulSoup(content, 'html.parser')
            pages = []
            
            # ä¸»é¡µ
            pages.append({
                "loc": f"{self.base_url}/",
                "lastmod": self.current_time,
                "changefreq": "weekly",
                "priority": "1.0",
                "title": "é¦–é¡µ"
            })
            
            # æŸ¥æ‰¾æ‰€æœ‰sectionå…ƒç´ 
            sections = soup.find_all('section', id=True)
            
            for section in sections:
                section_id = section.get('id')
                if section_id:
                    # è·å–sectionæ ‡é¢˜
                    title_elem = section.find(['h1', 'h2', 'h3'])
                    title = title_elem.get_text().strip() if title_elem else section_id.replace('-', ' ').title()
                    
                    # æ ¹æ®sectionç±»å‹è®¾ç½®ä¼˜å…ˆçº§å’Œæ›´æ–°é¢‘ç‡
                    priority, changefreq = self._get_section_priority(section_id, section)
                    
                    pages.append({
                        "loc": f"{self.base_url}/#{section_id}",
                        "lastmod": self.current_time,
                        "changefreq": changefreq,
                        "priority": priority,
                        "title": title
                    })
            
            # æŸ¥æ‰¾å¯¼èˆªé“¾æ¥
            nav_links = soup.find_all('a', href=re.compile(r'^#'))
            for link in nav_links:
                href = link.get('href')
                if href and href.startswith('#') and len(href) > 1:
                    section_id = href[1:]  # ç§»é™¤#å·
                    
                    # æ£€æŸ¥æ˜¯å¦å·²ç»æ·»åŠ è¿‡è¿™ä¸ªé“¾æ¥
                    if not any(page['loc'].endswith(f'#{section_id}') for page in pages):
                        title = link.get_text().strip() or section_id.replace('-', ' ').title()
                        priority, changefreq = self._get_section_priority(section_id)
                        
                        pages.append({
                            "loc": f"{self.base_url}/{href}",
                            "lastmod": self.current_time,
                            "changefreq": changefreq,
                            "priority": priority,
                            "title": title
                        })
            
            return pages
            
        except Exception as e:
            print(f"âŒ è§£æHTMLæ–‡ä»¶æ—¶å‡ºé”™: {e}")
            return []
    
    def _get_section_priority(self, section_id, section_elem=None):
        """æ ¹æ®sectionç±»å‹ç¡®å®šä¼˜å…ˆçº§å’Œæ›´æ–°é¢‘ç‡"""
        priority_map = {
            'about': ('0.9', 'monthly'),
            'about-me': ('0.9', 'monthly'),
            'publications': ('0.9', 'monthly'),
            'papers': ('0.9', 'monthly'),
            'research': ('0.9', 'monthly'),
            'talks': ('0.8', 'monthly'),
            'presentations': ('0.8', 'monthly'),
            'projects': ('0.8', 'monthly'),
            'experience': ('0.8', 'monthly'),
            'education': ('0.8', 'monthly'),
            'skills': ('0.7', 'monthly'),
            'tech-stack': ('0.7', 'monthly'),
            'contact': ('0.6', 'yearly'),
            'activities': ('0.7', 'monthly'),
            'academic-activities': ('0.7', 'monthly'),
            'awards': ('0.7', 'monthly'),
            'blog': ('0.8', 'weekly'),
            'news': ('0.8', 'weekly'),
        }
        
        # æ£€æŸ¥section_idæ˜¯å¦åŒ…å«å…³é”®è¯
        for keyword, (priority, changefreq) in priority_map.items():
            if keyword in section_id.lower():
                return priority, changefreq
        
        # å¦‚æœæœ‰sectionå…ƒç´ ï¼Œæ£€æŸ¥å…¶å†…å®¹
        if section_elem:
            # æ£€æŸ¥æ˜¯å¦åŒ…å«publicationç›¸å…³å†…å®¹
            if section_elem.find_all(['article', 'div'], class_=re.compile(r'publication|paper')):
                return '0.9', 'monthly'
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«projectç›¸å…³å†…å®¹
            if section_elem.find_all(['div'], class_=re.compile(r'project|work')):
                return '0.8', 'monthly'
        
        # é»˜è®¤å€¼
        return '0.7', 'monthly'
    
    def generate_sitemap(self):
        """ç”Ÿæˆsitemap.xmlæ–‡ä»¶"""
        print("ğŸ” æ­£åœ¨è§£æHTMLæ–‡ä»¶...")
        pages = self.parse_html_content()
        
        if not pages:
            print("âŒ æœªæ‰¾åˆ°ä»»ä½•é¡µé¢å†…å®¹")
            return False
        
        # å»é‡
        unique_pages = []
        seen_urls = set()
        for page in pages:
            if page['loc'] not in seen_urls:
                unique_pages.append(page)
                seen_urls.add(page['loc'])
        
        print(f"ğŸ“„ æ‰¾åˆ° {len(unique_pages)} ä¸ªå”¯ä¸€é¡µé¢")
        
        # åˆ›å»ºXMLç»“æ„
        urlset = ET.Element("urlset")
        urlset.set("xmlns", "http://www.sitemaps.org/schemas/sitemap/0.9")
        urlset.set("xmlns:xsi", "http://www.w3.org/2001/XMLSchema-instance")
        urlset.set("xsi:schemaLocation", "http://www.sitemaps.org/schemas/sitemap/0.9 http://www.sitemaps.org/schemas/sitemap/0.9/sitemap.xsd")
        
        # æŒ‰ä¼˜å…ˆçº§æ’åº
        unique_pages.sort(key=lambda x: float(x['priority']), reverse=True)
        
        # ä¸ºæ¯ä¸ªé¡µé¢åˆ›å»ºURLå…ƒç´ 
        for page in unique_pages:
            url = ET.SubElement(urlset, "url")
            
            loc = ET.SubElement(url, "loc")
            loc.text = page["loc"]
            
            lastmod = ET.SubElement(url, "lastmod")
            lastmod.text = page["lastmod"]
            
            changefreq = ET.SubElement(url, "changefreq")
            changefreq.text = page["changefreq"]
            
            priority = ET.SubElement(url, "priority")
            priority.text = page["priority"]
        
        # åˆ›å»ºXMLæ ‘å¹¶æ ¼å¼åŒ–
        tree = ET.ElementTree(urlset)
        ET.indent(tree, space="  ", level=0)
        
        # å†™å…¥æ–‡ä»¶
        tree.write("sitemap.xml", encoding="utf-8", xml_declaration=True)
        
        print(f"âœ… sitemap.xml å·²ç”ŸæˆæˆåŠŸï¼")
        print(f"ğŸ“… ç”Ÿæˆæ—¶é—´: {self.current_time}")
        print(f"ğŸ“„ åŒ…å« {len(unique_pages)} ä¸ªé¡µé¢")
        
        # æ˜¾ç¤ºç”Ÿæˆçš„é¡µé¢åˆ—è¡¨
        print("\nğŸ“‹ åŒ…å«çš„é¡µé¢:")
        for i, page in enumerate(unique_pages, 1):
            print(f"  {i}. {page['title']} - {page['loc']} (ä¼˜å…ˆçº§: {page['priority']})")
        
        # ä¿å­˜é¡µé¢ä¿¡æ¯åˆ°JSONæ–‡ä»¶ï¼ˆç”¨äºè°ƒè¯•ï¼‰
        with open("sitemap_pages.json", "w", encoding="utf-8") as f:
            json.dump(unique_pages, f, ensure_ascii=False, indent=2)
        
        return True
    
    def generate_robots_txt(self):
        """ç”Ÿæˆrobots.txtæ–‡ä»¶"""
        robots_content = f"""User-agent: *
Allow: /

# Sitemap
Sitemap: {self.base_url}/sitemap.xml

# ç¦æ­¢è®¿é—®çš„ç›®å½•ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
# Disallow: /admin/
# Disallow: /private/

# çˆ¬å–å»¶è¿Ÿï¼ˆå¯é€‰ï¼‰
Crawl-delay: 1
"""
        
        with open("robots.txt", "w", encoding="utf-8") as f:
            f.write(robots_content)
        
        print("âœ… robots.txt å·²ç”ŸæˆæˆåŠŸï¼")
    
    def validate_sitemap(self):
        """éªŒè¯ç”Ÿæˆçš„sitemap.xmlæ–‡ä»¶"""
        if not os.path.exists("sitemap.xml"):
            print("âŒ sitemap.xml æ–‡ä»¶ä¸å­˜åœ¨")
            return False
        
        try:
            tree = ET.parse("sitemap.xml")
            root = tree.getroot()
            
            # æ£€æŸ¥æ ¹å…ƒç´ 
            if root.tag != "{http://www.sitemaps.org/schemas/sitemap/0.9}urlset":
                print("âŒ sitemap.xml æ ¼å¼é”™è¯¯ï¼šæ ¹å…ƒç´ ä¸æ­£ç¡®")
                return False
            
            # ç»Ÿè®¡URLæ•°é‡
            urls = root.findall(".//{http://www.sitemaps.org/schemas/sitemap/0.9}url")
            
            # æ£€æŸ¥æ¯ä¸ªURLçš„å®Œæ•´æ€§
            valid_urls = 0
            for url in urls:
                loc = url.find("{http://www.sitemaps.org/schemas/sitemap/0.9}loc")
                lastmod = url.find("{http://www.sitemaps.org/schemas/sitemap/0.9}lastmod")
                changefreq = url.find("{http://www.sitemaps.org/schemas/sitemap/0.9}changefreq")
                priority = url.find("{http://www.sitemaps.org/schemas/sitemap/0.9}priority")
                
                if all(elem is not None for elem in [loc, lastmod, changefreq, priority]):
                    valid_urls += 1
            
            print(f"âœ… sitemap.xml éªŒè¯é€šè¿‡ï¼")
            print(f"ğŸ“Š æ€»URLæ•°é‡: {len(urls)}")
            print(f"ğŸ“Š æœ‰æ•ˆURLæ•°é‡: {valid_urls}")
            
            return True
            
        except ET.ParseError as e:
            print(f"âŒ sitemap.xml è§£æé”™è¯¯: {e}")
            return False

def main():
    print("ğŸš€ æ™ºèƒ½ç½‘ç«™åœ°å›¾ç”Ÿæˆå™¨")
    print("=" * 60)
    
    # é…ç½®å‚æ•°
    BASE_URL = "https://yourdomain.com"  # è¯·ä¿®æ”¹ä¸ºä½ çš„å®é™…åŸŸå
    HTML_FILE = "index.html"
    
    # æ£€æŸ¥æ˜¯å¦å­˜åœ¨é…ç½®æ–‡ä»¶
    config_file = "sitemap_config.json"
    if os.path.exists(config_file):
        try:
            with open(config_file, 'r', encoding='utf-8') as f:
                config = json.load(f)
                BASE_URL = config.get('base_url', BASE_URL)
                HTML_FILE = config.get('html_file', HTML_FILE)
            print(f"ğŸ“ å·²åŠ è½½é…ç½®æ–‡ä»¶: {config_file}")
        except:
            print("âš ï¸  é…ç½®æ–‡ä»¶æ ¼å¼é”™è¯¯ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
    
    print(f"ğŸŒ ç½‘ç«™URL: {BASE_URL}")
    print(f"ğŸ“„ HTMLæ–‡ä»¶: {HTML_FILE}")
    print("-" * 60)
    
    # åˆ›å»ºç”Ÿæˆå™¨å®ä¾‹
    generator = SitemapGenerator(BASE_URL, HTML_FILE)
    
    # ç”Ÿæˆsitemap
    if generator.generate_sitemap():
        print("\n" + "-" * 60)
        print("ğŸ” éªŒè¯ç”Ÿæˆçš„sitemap...")
        generator.validate_sitemap()
        
        print("\n" + "-" * 60)
        print("ğŸ¤– ç”Ÿæˆrobots.txt...")
        generator.generate_robots_txt()
    
    print("\n" + "=" * 60)
    print("ğŸ’¡ ä½¿ç”¨æç¤º:")
    print("1. ä¿®æ”¹è„šæœ¬ä¸­çš„ BASE_URL ä¸ºä½ çš„å®é™…åŸŸå")
    print("2. ç¡®ä¿ index.html æ–‡ä»¶å­˜åœ¨ä¸”æ ¼å¼æ­£ç¡®")
    print("3. å°†ç”Ÿæˆçš„æ–‡ä»¶ä¸Šä¼ åˆ°ç½‘ç«™æ ¹ç›®å½•:")
    print("   - sitemap.xml")
    print("   - robots.txt")
    print("4. åœ¨ Google Search Console ä¸­æäº¤sitemap")
    print("5. å¯ä»¥åˆ›å»º sitemap_config.json é…ç½®æ–‡ä»¶:")
    print('   {"base_url": "https://yoursite.com", "html_file": "index.html"}')

if __name__ == "__main__":
    main() 